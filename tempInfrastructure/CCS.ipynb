{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fire\n",
    "import json\n",
    "import os\n",
    "import nltk\n",
    "import gensim\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from nltk import tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from gpt2model import model, encoder, sample\n",
    "\n",
    "# Mute tf WARNING messages\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "Gmodel = gensim.models.KeyedVectors.load_word2vec_format('./GoogleNewsModel/GoogleNews-vectors-negative300.bin', binary=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interact_model will take a string, as prompt and tries to return a related text \n",
    "\n",
    "\n",
    "def interact_model(raw_text,\n",
    "    model_name='774M',\n",
    "    seed=None,\n",
    "    nsamples=1,\n",
    "    batch_size=1,\n",
    "    length=None,\n",
    "    temperature=1,\n",
    "    top_k=40,\n",
    "    top_p=1,\n",
    "    models_dir='gpt2model',\n",
    "):\n",
    "    \"\"\"\n",
    "    Interactively run the model\n",
    "    :model_name=124M : String, which model to use\n",
    "    :seed=None : Integer seed for random number generators, fix seed to reproduce\n",
    "     results\n",
    "    :nsamples=1 : Number of samples to return total\n",
    "    :batch_size=1 : Number of batches (only affects speed/memory).  Must divide nsamples.\n",
    "    :length=None : Number of tokens in generated text, if None (default), is\n",
    "     determined by model hyperparameters\n",
    "    :temperature=1 : Float value controlling randomness in boltzmann\n",
    "     distribution. Lower temperature results in less random completions. As the\n",
    "     temperature approaches zero, the model will become deterministic and\n",
    "     repetitive. Higher temperature results in more random completions.\n",
    "    :top_k=0 : Integer value controlling diversity. 1 means only 1 word is\n",
    "     considered for each step (token), resulting in deterministic completions,\n",
    "     while 40 means 40 words are considered at each step. 0 (default) is a\n",
    "     special setting meaning no restrictions. 40 generally is a good value.\n",
    "     :models_dir : path to parent folder containing model subfolders\n",
    "     (i.e. contains the <model_name> folder)\n",
    "    \"\"\"\n",
    "    models_dir = os.path.expanduser(os.path.expandvars(models_dir))\n",
    "    if batch_size is None:\n",
    "        batch_size = 1\n",
    "    assert nsamples % batch_size == 0\n",
    "\n",
    "    enc = encoder.get_encoder(model_name, models_dir)\n",
    "    hparams = model.default_hparams()\n",
    "    with open(os.path.join(models_dir, model_name, 'hparams.json')) as f:\n",
    "        hparams.override_from_dict(json.load(f))\n",
    "\n",
    "    if length is None:\n",
    "        length = hparams.n_ctx // 2\n",
    "    elif length > hparams.n_ctx:\n",
    "        raise ValueError(\"Can't get samples longer than window size: %s\" % hparams.n_ctx)\n",
    "\n",
    "    with tf.Session(graph=tf.Graph()) as sess:\n",
    "        context = tf.placeholder(tf.int32, [batch_size, None])\n",
    "        np.random.seed(seed)\n",
    "        tf.set_random_seed(seed)\n",
    "        output = sample.sample_sequence(\n",
    "            hparams=hparams, length=length,\n",
    "            context=context,\n",
    "            batch_size=batch_size,\n",
    "            temperature=temperature, top_k=top_k, top_p=top_p\n",
    "        )\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "        ckpt = tf.train.latest_checkpoint(os.path.join(models_dir, model_name))\n",
    "        saver.restore(sess, ckpt)\n",
    "\n",
    "        \n",
    "        context_tokens = enc.encode(raw_text)\n",
    "        generated = 0\n",
    "        for _ in range(nsamples // batch_size):\n",
    "            out = sess.run(output, feed_dict={\n",
    "                context: [context_tokens for _ in range(batch_size)]\n",
    "            })[:, len(context_tokens):]\n",
    "            for i in range(batch_size):\n",
    "                generated += 1\n",
    "                text = enc.decode(out[i])\n",
    "                # here gpt2 returns the output\n",
    "                print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the GPT2 Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "My cat looks like someone put a cat in a box and it's been running around.\n",
      "\n",
      "It's kind of cute like you can't help but want to pet it though.\n",
      "\n",
      "The cat is definitely a lot smaller than I thought it was when it moved into my living room. I'd never seen them before.\n",
      "\n",
      "In my house, it used to be a large rat, but we got rid of it.\n",
      "\n",
      "We got a new cat. He's tiny. He doesn't even have a tail.\n",
      "\n",
      "It has two small ears though. I'll still find a way to pet him.\n",
      "\n",
      "The cat's eyes are a little bigger than yours, aren't they? Not as big as yours are, but there is still some room in them for them to get big.\n",
      "\n",
      "The cat doesn't seem aggressive at all, but there is a hint of cat aggression in its behaviour.\n",
      "\n",
      "My wife and I can't really explain the cat, but we definitely see it as a different animal. I think it's just like us. It may act friendly or aggressive, but there isn't a trace of it on our first day.\n",
      "\n",
      "When we went out for the first time, we weren't expecting an attack from a cat.\n",
      "\n",
      "We saw it sitting on my dresser, but had no idea what kind. We hadn't seen anything like that before.\n",
      "\n",
      "The cat didn't give us any trouble at all, but there is definitely a little more aggression in its acts now. It's definitely different from the time when we were together.\n",
      "\n",
      "I love pets that are friendly and are kind.\n",
      "\n",
      "There are lots of cats in my neighbourhood. It's all the cats playing together in the yard.\n",
      "\n",
      "When it walks in and sees its friend or neighbour, it will jump up and chase the other cat.\n",
      "\n",
      "It probably doesn't like to be around humans, but we just took it aside one day. I didn't want to go. Maybe we're not such great with cats and it's been trying to find a new home in our city. At this rate, it could end up in our city as well.\n",
      "\n",
      "The first time we had a cat, I was actually worried it would be a good thing to have.\n",
      "\n",
      "Now that we have a cat, I don't know.\n",
      "\n",
      "This cat didn't attack any of my family members, but it did take a bite out of one of them. Maybe it didn\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(interact_model('Cats are cute.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google W2V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deal with stopwords and punctuation marks\n",
    "stopwords_list = nltk.corpus.stopwords.words('english')\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_list(sent):\n",
    "    '''\n",
    "    gets sentence returns a list of words\n",
    "    '''\n",
    "    # tokenize the input sentence to a list\n",
    "    sent_list = tokenizer.tokenize(sent)\n",
    "    # and remove the stopwords\n",
    "    sent_list = [word for word in sent_list if (not(word in stopwords_list) and word in Gmodel.vocab)]\n",
    "    \n",
    "    return sent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_sents(text):\n",
    "    '''\n",
    "    gets text returns a list of sentences\n",
    "    '''\n",
    "    # tokenize the gpt2 output\n",
    "    sentences = tokenize.sent_tokenize(text.lower()) # to sentences\n",
    "        \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22234654\n",
      "0.13977832\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{\" dogs are cute with toys, they''re cute at their homes and all the other fun things people do to foster that connection.\": 0.22234654}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< HEAD
    "adef get_candidates(text, goal_word):\n",
    "    '''\n",
    "    takes a text and a goal word and returns\n",
    "    the closest sentence from the text to the goal word\n",
    "    '''\n",
=======
    "def get_candidates(text, goal_word):\n",
    "    # if input is a string, split it into sentences\n",
>>>>>>> 921fc02e092edee561f99525eb73748b6b3127b6
    "    if isinstance(text, str):\n",
    "        text = text_to_sents(text)\n",
    "    candidates = {}\n",
    "    print(text)\n",
    "    for sent in text:\n",
    "        sent_list = sent_to_list(sent)\n",
    "        sim = Gmodel.n_similarity(sent_list, [goal_word])\n",
    "        print(sent, sim)\n",
    "    # TODO tokenize the sentences\n",
    "    # similarity between current sentence and goal_word (must be updated)\n",
    "    current_sim = 0.0\n",
<<<<<<< HEAD
    "    # for each sentence:\n",
    "    #   get similarity between it and goal word\n",
    "    #   if this_sim > current_sim:\n",
    "    #       candidates.append(sentence : similarity)\n",
    "    # return candidates\n",
=======
    "    for sent in text:\n",
    "        sim = Gmodel.n_similarity(sent_to_list(sent), [goal_word])\n",
    "        print(sim)\n",
    "        if sim > current_sim:\n",
    "            candidates[sent] = sim\n",
    "            current_sim = sim\n",
    "        \n",
    "    return candidates\n",
    "\n",
>>>>>>> 921fc02e092edee561f99525eb73748b6b3127b6
    "get_candidates(\" Dogs are cute with toys, they''re cute at their homes and all the other fun things people do to foster that connection. It''s just that many, many people, and I suspect you can find it in the internet as well, don''t want to feel a connection with their companion animal in certain ways.\", \"spaceship\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
