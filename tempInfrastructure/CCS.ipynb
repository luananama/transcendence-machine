{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fire\n",
    "import json\n",
    "import os\n",
    "import nltk\n",
    "import gensim\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from nltk import tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from gpt2model import model, encoder, sample\n",
    "\n",
    "# Mute tf WARNING messages\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "Gmodel = gensim.models.KeyedVectors.load_word2vec_format('./GoogleNewsModel/GoogleNews-vectors-negative300.bin', binary=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interact_model will take a string, as prompt and tries to return a related text \n",
    "\n",
    "\n",
    "def interact_model(raw_text,\n",
    "    model_name='774M',\n",
    "    seed=None,\n",
    "    nsamples=1,\n",
    "    batch_size=1,\n",
    "    length=None,\n",
    "    temperature=1,\n",
    "    top_k=40,\n",
    "    top_p=1,\n",
    "    models_dir='gpt2model',\n",
    "):\n",
    "    \"\"\"\n",
    "    Interactively run the model\n",
    "    :model_name=124M : String, which model to use\n",
    "    :seed=None : Integer seed for random number generators, fix seed to reproduce\n",
    "     results\n",
    "    :nsamples=1 : Number of samples to return total\n",
    "    :batch_size=1 : Number of batches (only affects speed/memory).  Must divide nsamples.\n",
    "    :length=None : Number of tokens in generated text, if None (default), is\n",
    "     determined by model hyperparameters\n",
    "    :temperature=1 : Float value controlling randomness in boltzmann\n",
    "     distribution. Lower temperature results in less random completions. As the\n",
    "     temperature approaches zero, the model will become deterministic and\n",
    "     repetitive. Higher temperature results in more random completions.\n",
    "    :top_k=0 : Integer value controlling diversity. 1 means only 1 word is\n",
    "     considered for each step (token), resulting in deterministic completions,\n",
    "     while 40 means 40 words are considered at each step. 0 (default) is a\n",
    "     special setting meaning no restrictions. 40 generally is a good value.\n",
    "     :models_dir : path to parent folder containing model subfolders\n",
    "     (i.e. contains the <model_name> folder)\n",
    "    \"\"\"\n",
    "    models_dir = os.path.expanduser(os.path.expandvars(models_dir))\n",
    "    if batch_size is None:\n",
    "        batch_size = 1\n",
    "    assert nsamples % batch_size == 0\n",
    "\n",
    "    enc = encoder.get_encoder(model_name, models_dir)\n",
    "    hparams = model.default_hparams()\n",
    "    with open(os.path.join(models_dir, model_name, 'hparams.json')) as f:\n",
    "        hparams.override_from_dict(json.load(f))\n",
    "\n",
    "    if length is None:\n",
    "        length = hparams.n_ctx // 2\n",
    "    elif length > hparams.n_ctx:\n",
    "        raise ValueError(\"Can't get samples longer than window size: %s\" % hparams.n_ctx)\n",
    "\n",
    "    with tf.Session(graph=tf.Graph()) as sess:\n",
    "        context = tf.placeholder(tf.int32, [batch_size, None])\n",
    "        np.random.seed(seed)\n",
    "        tf.set_random_seed(seed)\n",
    "        output = sample.sample_sequence(\n",
    "            hparams=hparams, length=length,\n",
    "            context=context,\n",
    "            batch_size=batch_size,\n",
    "            temperature=temperature, top_k=top_k, top_p=top_p\n",
    "        )\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "        ckpt = tf.train.latest_checkpoint(os.path.join(models_dir, model_name))\n",
    "        saver.restore(sess, ckpt)\n",
    "\n",
    "        \n",
    "        context_tokens = enc.encode(raw_text)\n",
    "        generated = 0\n",
    "        for _ in range(nsamples // batch_size):\n",
    "            out = sess.run(output, feed_dict={\n",
    "                context: [context_tokens for _ in range(batch_size)]\n",
    "            })[:, len(context_tokens):]\n",
    "            for i in range(batch_size):\n",
    "                generated += 1\n",
    "                text = enc.decode(out[i])\n",
    "                # here gpt2 returns the output\n",
    "                print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the GPT2 Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "It's not my story!\n",
      "\n",
      "\n",
      "To: corkid\n",
      "\n",
      "\"This is the type of stuff you know\" -\n",
      "\n",
      "\n",
      "To: corkid\n",
      "\n",
      "The president of the United States is a cat. That is not even close to the worst thing that could happen to a Republican. If they were not a Republican, I would think I'd get more support...\n",
      "\n",
      "\n",
      "by 15 posted onby R.M. (Pray For A President who will \"put the country FIRST.\")\n",
      "\n",
      "To: corkid\n",
      "\n",
      "\n",
      "by 16 posted onby kludzie (\"If the government is not responsive, then we don't need a government\" - FDR\n",
      "\n",
      "\n",
      "To: corkid\n",
      "\n",
      "To: corkid\n",
      "\n",
      "\n",
      "by 17 posted onby Red (The New Jersey governor has the largest percentage of votes of any candidate of the party nominated by each party.\")\n",
      "\n",
      "To: corkid\n",
      "\n",
      "\"If this guy is the Republican, I think the Republicans will lose the election.\" This is so not true. As is shown in the past, the party has consistently and consistently failed to win a Presidential election. To say that the president is a cat is absurd, and you can see it for yourself if you look up the definition of Cat.\n",
      "\n",
      "\n",
      "To: corkid\n",
      "\n",
      "They do exist as the political parties go through cycles: if one party is unpopular at the start, it tends to win the election. But they are generally dead, while the opposing party has a stronger chance of victory\n",
      "\n",
      "\n",
      "To: corkid\n",
      "\n",
      "I guess he's right about the President being a small animal......\n",
      "\n",
      "\n",
      "by 22 posted onby ladd (They think they're so secure - they don't know it yet)\n",
      "\n",
      "To: corkid\n",
      "\n",
      "The problem with the president's cats is that they are too old and frail to be a good role model, so they end up in a lot of stories. Like this one on CNN, that's just so lame\n",
      "\n",
      "\n",
      "by 23 posted onby mrb (They lie but keep on lying)\n",
      "\n",
      "To: kludzie\n",
      "\n",
      "I'm not entirely sure what is so different about a cat from a dog, but there are certain things that a cat can do that dogs can't:\n",
      "\n",
      "A cat is a companion. A dog does not have to be able to be petted. A cat is capable of making all of the noises and grunts that humans are capable of making.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(interact_model('Cats are cute.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google W2V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
