{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fire\n",
    "import json\n",
    "import os\n",
    "import nltk\n",
    "import gensim\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from nltk import tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from gpt2model import model, encoder, sample\n",
    "\n",
    "# Mute tf WARNING messages\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "Gmodel = gensim.models.KeyedVectors.load_word2vec_format('./GoogleNewsModel/GoogleNews-vectors-negative300.bin', binary=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interact_model will take a string, as prompt and tries to return a related text \n",
    "\n",
    "\n",
    "def interact_model(raw_text,\n",
    "    model_name='774M',\n",
    "    seed=None,\n",
    "    nsamples=1,\n",
    "    batch_size=1,\n",
    "    length=None,\n",
    "    temperature=1,\n",
    "    top_k=40,\n",
    "    top_p=1,\n",
    "    models_dir='gpt2model',\n",
    "):\n",
    "    \"\"\"\n",
    "    Interactively run the model\n",
    "    :model_name=124M : String, which model to use\n",
    "    :seed=None : Integer seed for random number generators, fix seed to reproduce\n",
    "     results\n",
    "    :nsamples=1 : Number of samples to return total\n",
    "    :batch_size=1 : Number of batches (only affects speed/memory).  Must divide nsamples.\n",
    "    :length=None : Number of tokens in generated text, if None (default), is\n",
    "     determined by model hyperparameters\n",
    "    :temperature=1 : Float value controlling randomness in boltzmann\n",
    "     distribution. Lower temperature results in less random completions. As the\n",
    "     temperature approaches zero, the model will become deterministic and\n",
    "     repetitive. Higher temperature results in more random completions.\n",
    "    :top_k=0 : Integer value controlling diversity. 1 means only 1 word is\n",
    "     considered for each step (token), resulting in deterministic completions,\n",
    "     while 40 means 40 words are considered at each step. 0 (default) is a\n",
    "     special setting meaning no restrictions. 40 generally is a good value.\n",
    "     :models_dir : path to parent folder containing model subfolders\n",
    "     (i.e. contains the <model_name> folder)\n",
    "    \"\"\"\n",
    "    models_dir = os.path.expanduser(os.path.expandvars(models_dir))\n",
    "    if batch_size is None:\n",
    "        batch_size = 1\n",
    "    assert nsamples % batch_size == 0\n",
    "\n",
    "    enc = encoder.get_encoder(model_name, models_dir)\n",
    "    hparams = model.default_hparams()\n",
    "    with open(os.path.join(models_dir, model_name, 'hparams.json')) as f:\n",
    "        hparams.override_from_dict(json.load(f))\n",
    "\n",
    "    if length is None:\n",
    "        length = hparams.n_ctx // 2\n",
    "    elif length > hparams.n_ctx:\n",
    "        raise ValueError(\"Can't get samples longer than window size: %s\" % hparams.n_ctx)\n",
    "\n",
    "    with tf.Session(graph=tf.Graph()) as sess:\n",
    "        context = tf.placeholder(tf.int32, [batch_size, None])\n",
    "        np.random.seed(seed)\n",
    "        tf.set_random_seed(seed)\n",
    "        output = sample.sample_sequence(\n",
    "            hparams=hparams, length=length,\n",
    "            context=context,\n",
    "            batch_size=batch_size,\n",
    "            temperature=temperature, top_k=top_k, top_p=top_p\n",
    "        )\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "        ckpt = tf.train.latest_checkpoint(os.path.join(models_dir, model_name))\n",
    "        saver.restore(sess, ckpt)\n",
    "\n",
    "        \n",
    "        context_tokens = enc.encode(raw_text)\n",
    "        generated = 0\n",
    "        for _ in range(nsamples // batch_size):\n",
    "            out = sess.run(output, feed_dict={\n",
    "                context: [context_tokens for _ in range(batch_size)]\n",
    "            })[:, len(context_tokens):]\n",
    "            for i in range(batch_size):\n",
    "                generated += 1\n",
    "                text = enc.decode(out[i])\n",
    "                # here gpt2 returns the output\n",
    "                print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the GPT2 Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(interact_model('Cats are cute.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google W2V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deal with stopwords and punctuation marks\n",
    "stopwords_list = nltk.corpus.stopwords.words('english')\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_list(sent):\n",
    "    '''\n",
    "    gets sentence returns a list of words\n",
    "    '''\n",
    "    # tokenize the input sentence to a list\n",
    "    sent_list = tokenizer.tokenize(sent)\n",
    "    # and remove the stopwords\n",
    "    sent_list = [word for word in sent_list if (not(word in stopwords_list) and word in Gmodel.vocab)]\n",
    "    \n",
    "    return sent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_sents(text):\n",
    "    '''\n",
    "    gets text returns a list of sentences\n",
    "    '''\n",
    "    # tokenize the gpt2 output\n",
    "    sentences = tokenize.sent_tokenize(text.lower()) # to sentences\n",
    "    \n",
    "#     for i in range(len(sentences)): \n",
    "#         sentences[i] = sent_to_list(sentences[i])\n",
    "        \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\" dogs are cute with toys, they''re cute at their homes and all the other fun things people do to foster that connection.\", \"it''s just that many, many people, and i suspect you can find it in the internet as well, don''t want to feel a connection with their companion animal in certain ways.\"]\n",
      " dogs are cute with toys, they''re cute at their homes and all the other fun things people do to foster that connection. 0.22234654\n",
      "it''s just that many, many people, and i suspect you can find it in the internet as well, don''t want to feel a connection with their companion animal in certain ways. 0.13977832\n"
     ]
    }
   ],
   "source": [
    "def get_candidates(text, goal_word):\n",
    "    # if input is a string, split it into sentences\n",
    "    if type(text) == 'str':\n",
    "        text = text_to_sents(text)\n",
    "    candidates = {}\n",
    "    # similarity between current sentence and goal_word (must be updated)\n",
    "    current_sim = 0.0\n",
    "    for sent in text:\n",
    "        sim = model.n_similarity(sent_to_list(), [goal_word])\n",
    "        if sim > current_sim:\n",
    "            candidates[sent] = sim\n",
    "            current_sim = sim\n",
    "        \n",
    "    return candidates\n",
    "\n",
    "get_candidates(\" Dogs are cute with toys, they''re cute at their homes and all the other fun things people do to foster that connection. It''s just that many, many people, and I suspect you can find it in the internet as well, don''t want to feel a connection with their companion animal in certain ways.\", \"spaceship\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
